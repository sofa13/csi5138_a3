{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CSI 5138 Homework 3**\n",
    "\n",
    "Use Vanilla RNN and LSTM to for text classification and sentiment analysis on a standard dataset of movie reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Embedding, Dense, Flatten\n",
    "from keras.layers import Input, LSTM, Dropout, SimpleRNN\n",
    "from keras.models import Sequential, Model\n",
    "from keras import optimizers\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Movie Review Dataset, Test and Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processReviews(paths):\n",
    "    texts = []\n",
    "    ratings = []\n",
    "    \n",
    "    for path in paths:\n",
    "        for file in os.listdir(path):\n",
    "            # get review\n",
    "            rating = file.split('_')[1]\n",
    "            rating = rating.split('.')[0]\n",
    "            file = os.path.join(path, file)\n",
    "            with open(file, \"r\", encoding='utf-8') as f:\n",
    "                text = []\n",
    "                for line in f:\n",
    "                    # do some pre-processing and combine list of words for each review text             \n",
    "                    text += gensim.utils.simple_preprocess(line)\n",
    "                texts.append(text)\n",
    "                ratings.append(rating)\n",
    "        \n",
    "    return [texts, ratings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, ytrain = processReviews([\"./aclImdb/train/neg/\", \"./aclImdb/train/pos/\"])\n",
    "Xtest, ytest = processReviews([\"./aclImdb/test/neg/\", \"./aclImdb/test/pos/\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file\n",
    "with open('gensim', 'w', encoding=\"utf-8\") as fout:\n",
    "    pprint([Xtrain, ytrain, Xtest, ytest], fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from file\n",
    "with open('gensim', 'r', encoding=\"utf-8\") as fin:\n",
    "    content = eval(fin.read())\n",
    "\n",
    "Xtrain, ytrain, Xtest, ytest = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['story', 'of', 'man', 'who', 'has']\n",
      "3\n",
      "['once', 'again', 'mr', 'costner', 'has']\n",
      "2\n",
      "['bromwell', 'high', 'is', 'cartoon', 'comedy']\n",
      "9\n",
      "['went', 'and', 'saw', 'this', 'movie']\n",
      "10\n",
      "# Xtrain:  25000\n",
      "# ytrain:  25000\n",
      "# Xtest:  25000\n",
      "# ytest:  25000\n"
     ]
    }
   ],
   "source": [
    "print(Xtrain[0][:5])\n",
    "print(ytrain[0])\n",
    "print(Xtest[0][:5])\n",
    "print(ytest[0])\n",
    "print(Xtrain[12500][:5])\n",
    "print(ytrain[12500])\n",
    "print(Xtest[12500][:5])\n",
    "print(ytest[12500])\n",
    "print(\"# Xtrain: \", len(Xtrain))\n",
    "print(\"# ytrain: \", len(ytrain))\n",
    "print(\"# Xtest: \", len(Xtest))\n",
    "print(\"# ytest: \", len(ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# X:  50000\n",
      "# y:  50000\n"
     ]
    }
   ],
   "source": [
    "X = list(Xtrain + Xtest)\n",
    "y = list(ytrain + ytest)\n",
    "print(\"# X: \", len(X))\n",
    "print(\"# y: \", len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3', '4', '4', '1', '1', '3', '3', '4', '1', '2']\n",
      "['10', '10', '8', '10', '10', '8', '10', '8', '10', '7']\n",
      "[False, False, False, False, False, False, False, False, False, False]\n",
      "[True, True, True, True, True, True, True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "# convert reviews to pos and negative\n",
    "# pos 1, neg 0\n",
    "print(y[:10])\n",
    "print(y[-10:])\n",
    "y = [int(a)>= 7 for a in y]\n",
    "print(y[:10])\n",
    "print(y[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Keras with Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 99476 unique tokens.\n",
      "Shape of data tensor: (50000, 500)\n",
      "Shape of label tensor: (50000, 2)\n",
      "[1. 0.]\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH=500\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, padding='post', maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(y))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.320e+02, 1.694e+03, 2.380e+03, 2.451e+03, 7.166e+03, 7.133e+03,\n",
       "        5.144e+03, 3.701e+03, 3.093e+03, 2.401e+03, 1.918e+03, 1.590e+03,\n",
       "        1.473e+03, 1.170e+03, 1.104e+03, 8.650e+02, 7.810e+02, 6.820e+02,\n",
       "        6.080e+02, 4.910e+02, 4.330e+02, 4.210e+02, 3.750e+02, 2.980e+02,\n",
       "        3.030e+02, 2.380e+02, 1.960e+02, 1.940e+02, 1.850e+02, 1.600e+02,\n",
       "        1.320e+02, 1.270e+02, 1.330e+02, 1.110e+02, 7.500e+01, 7.900e+01,\n",
       "        7.600e+01, 8.000e+01, 6.400e+01, 1.110e+02, 1.330e+02, 5.600e+01,\n",
       "        5.000e+00, 5.000e+00, 6.000e+00, 0.000e+00, 0.000e+00, 3.000e+00,\n",
       "        2.000e+00, 2.000e+00, 0.000e+00, 0.000e+00, 3.000e+00, 1.000e+00,\n",
       "        0.000e+00, 1.000e+00, 2.000e+00, 1.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 2.000e+00, 1.000e+00, 0.000e+00, 0.000e+00,\n",
       "        1.000e+00, 0.000e+00, 0.000e+00, 2.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 1.000e+00, 0.000e+00, 1.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 1.000e+00, 1.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 1.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 1.000e+00]),\n",
       " array([   6.  ,   29.74,   53.48,   77.22,  100.96,  124.7 ,  148.44,\n",
       "         172.18,  195.92,  219.66,  243.4 ,  267.14,  290.88,  314.62,\n",
       "         338.36,  362.1 ,  385.84,  409.58,  433.32,  457.06,  480.8 ,\n",
       "         504.54,  528.28,  552.02,  575.76,  599.5 ,  623.24,  646.98,\n",
       "         670.72,  694.46,  718.2 ,  741.94,  765.68,  789.42,  813.16,\n",
       "         836.9 ,  860.64,  884.38,  908.12,  931.86,  955.6 ,  979.34,\n",
       "        1003.08, 1026.82, 1050.56, 1074.3 , 1098.04, 1121.78, 1145.52,\n",
       "        1169.26, 1193.  , 1216.74, 1240.48, 1264.22, 1287.96, 1311.7 ,\n",
       "        1335.44, 1359.18, 1382.92, 1406.66, 1430.4 , 1454.14, 1477.88,\n",
       "        1501.62, 1525.36, 1549.1 , 1572.84, 1596.58, 1620.32, 1644.06,\n",
       "        1667.8 , 1691.54, 1715.28, 1739.02, 1762.76, 1786.5 , 1810.24,\n",
       "        1833.98, 1857.72, 1881.46, 1905.2 , 1928.94, 1952.68, 1976.42,\n",
       "        2000.16, 2023.9 , 2047.64, 2071.38, 2095.12, 2118.86, 2142.6 ,\n",
       "        2166.34, 2190.08, 2213.82, 2237.56, 2261.3 , 2285.04, 2308.78,\n",
       "        2332.52, 2356.26, 2380.  ]),\n",
       " <a list of 100 Patch objects>)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sequence_lens = sorted([len(s) for s in sequences])\n",
    "\n",
    "# plots histogram\n",
    "plt.hist(sequence_lens, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "glove_file = './glove.6B/glove.6B.50d.txt'\n",
    "\n",
    "with open(glove_file, \"r\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM=50\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99476\n",
      "99477\n"
     ]
    }
   ],
   "source": [
    "print(len(word_index))\n",
    "print(len(embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "682\n",
      "1577\n",
      "[ 0.50450999  0.68607002 -0.59517002 -0.022801    0.60045999]\n",
      "[ 0.37854001  1.8233     -1.26479995 -0.1043      0.35828999]\n",
      "[ 0.19475     0.60993999 -0.66619998  0.01851     1.37290001]\n",
      "[0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#print(list(word_index.items())[100])\n",
    "# sentence word_index = 101\n",
    "print(word_index['king'])\n",
    "print(word_index['queen'])\n",
    "print(embedding_matrix[682][:5])\n",
    "print(embedding_matrix[1577][:5])\n",
    "print(embedding_matrix[101][:5])\n",
    "print(embedding_matrix[0][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate train, val and test. Take test as half val, half test.\n",
    "X_train, y_train = data[:25000], labels[:25000]\n",
    "X_val, y_val = data[25000:37500], labels[25000:37500]\n",
    "X_test, y_test = data[37500:], labels[37500:]\n",
    "X_test, y_test = data[25000:], labels[25000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# x_train:  25000\n",
      "# x_val:  12500\n",
      "# x_test:  25000\n"
     ]
    }
   ],
   "source": [
    "print(\"# x_train: \", len(X_train))\n",
    "print(\"# x_val: \", len(X_val))\n",
    "print(\"# x_test: \", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_test, y_train_test = X_train[:50], y_train[:50]\n",
    "X_val_test, y_val_test = X_val[:50], y_val[:50]\n",
    "X_test_test, y_test_test = X_test[:10], y_test[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vanilla_rnn(num_words, state, lra, dropout, num_outputs=2, emb_dim=50, input_length=500):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=num_words + 1, output_dim=emb_dim, input_length=input_length, trainable=False, weights=[embedding_matrix]))\n",
    "    model.add(SimpleRNN(units=state, input_shape=(num_words,1), return_sequences=False))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(num_outputs, activation='sigmoid'))\n",
    "    \n",
    "    rmsprop = optimizers.RMSprop(lr = lra)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = rmsprop, metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_rnn(num_words, state, lra, dropout, num_outputs=2, emb_dim=50, input_length=500):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=num_words + 1, output_dim=emb_dim, input_length=input_length, trainable=False, weights=[embedding_matrix]))\n",
    "    model.add(LSTM(state))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(num_outputs, activation='sigmoid'))\n",
    "    \n",
    "    rmsprop = optimizers.RMSprop(lr = lra)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=rmsprop, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runModel(state, lr, batch, dropout, model, epoch=5, num_outputs=2, emb_dim=100, input_length=2380):\n",
    "        \n",
    "    num_words = len(word_index)\n",
    "    if model == \"lstm\": \n",
    "        model = lstm_rnn(num_words, state, lr, dropout)\n",
    "    elif model == \"vanilla\":\n",
    "        model = vanilla_rnn(num_words, state, lr, dropout)\n",
    "        \n",
    "    #model.summary()\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epoch, batch_size=batch, verbose=1)\n",
    "\n",
    "    testscore = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test loss:', testscore[0])\n",
    "    print('Test accuracy:', testscore[1])\n",
    "    \n",
    "    return [history.history, testscore]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypruns(state, comb, repeats, model):\n",
    "    history = []\n",
    "    testscore = []\n",
    "\n",
    "    for i in range(repeats):\n",
    "        l, b, d = comb\n",
    "        print(\"state %s, lr %s, batch %s, dropout %s.\" %(state, l, b, d))\n",
    "        res = runModel(state, l, b, d, model)\n",
    "        \n",
    "        if res:\n",
    "            history.append(res[0])\n",
    "            testscore.append(res[1])\n",
    "    \n",
    "    # take avg of testscore\n",
    "    testscore = list(np.mean(np.array(testscore), axis=0))\n",
    "    hyps = [state] + comb\n",
    "    \n",
    "    return [history, testscore, hyps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tunehyps(states, comb, repeats, model):\n",
    "    res = []\n",
    "    hist = []\n",
    "    for state in states:\n",
    "        for comb in combs:\n",
    "            history, testscore, hyps = hypruns(state, comb, repeats, model)\n",
    "            res.append(testscore + hyps)\n",
    "            hist.append(history)\n",
    "                \n",
    "        # save testscore to file\n",
    "        with open('./experiments/'+model+'/testscore_'+'state_'+str(state), 'w', encoding=\"utf-8\") as fout:\n",
    "            pprint(res, fout)\n",
    "\n",
    "        # save history to file\n",
    "        with open('./experiments/'+model+'/history_'+'state_'+str(state), 'w', encoding=\"utf-8\") as fout:\n",
    "            pprint(hist, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.001, 100, 0.5]]\n",
      "state 20, lr 0.001, batch 100, dropout 0.5.\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 99s 4ms/step - loss: 0.6939 - acc: 0.4978 - val_loss: 0.6923 - val_acc: 0.5070\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 80s 3ms/step - loss: 0.6917 - acc: 0.5076 - val_loss: 0.6899 - val_acc: 0.5127\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 79s 3ms/step - loss: 0.6878 - acc: 0.5147 - val_loss: 0.6845 - val_acc: 0.5243\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 80s 3ms/step - loss: 0.6556 - acc: 0.6240 - val_loss: 0.6277 - val_acc: 0.6736\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 79s 3ms/step - loss: 0.6317 - acc: 0.6764 - val_loss: 0.7026 - val_acc: 0.5492\n",
      "Test loss: 0.7025632075500489\n",
      "Test accuracy: 0.5492\n",
      "state 50, lr 0.001, batch 100, dropout 0.5.\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 209s 8ms/step - loss: 0.6925 - acc: 0.5151 - val_loss: 0.6609 - val_acc: 0.6606\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 203s 8ms/step - loss: 0.6556 - acc: 0.6414 - val_loss: 0.6307 - val_acc: 0.6645\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 203s 8ms/step - loss: 0.6700 - acc: 0.5949 - val_loss: 0.6293 - val_acc: 0.6803\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 204s 8ms/step - loss: 0.6403 - acc: 0.6572 - val_loss: 0.6154 - val_acc: 0.6835\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 204s 8ms/step - loss: 0.6394 - acc: 0.6551 - val_loss: 0.6630 - val_acc: 0.6236\n",
      "Test loss: 0.6629722957229615\n",
      "Test accuracy: 0.6236\n",
      "state 100, lr 0.001, batch 100, dropout 0.5.\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 1180s 47ms/step - loss: 0.6923 - acc: 0.5070 - val_loss: 0.6871 - val_acc: 0.5181\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 1403s 56ms/step - loss: 0.6869 - acc: 0.5205 - val_loss: 0.6934 - val_acc: 0.5072\n",
      "Epoch 3/5\n",
      "17300/25000 [===================>..........] - ETA: 7:09 - loss: 0.6694 - acc: 0.5970"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-195-31e9c71308eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcombs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mtunehyps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcombs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepeats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-191-08517d270122>\u001b[0m in \u001b[0;36mtunehyps\u001b[1;34m(states, comb, repeats, model)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcomb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcombs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m             \u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhyps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhypruns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepeats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m             \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestscore\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mhyps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mhist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-193-51f7ce170da9>\u001b[0m in \u001b[0;36mhypruns\u001b[1;34m(state, comb, repeats, model)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"state %s, lr %s, batch %s, dropout %s.\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrunModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-187-67646774ab14>\u001b[0m in \u001b[0;36mrunModel\u001b[1;34m(state, lr, batch, dropout, model, epoch, num_outputs, emb_dim, input_length)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m#model.summary()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mtestscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[1;32m-> 1451\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "states = [20, 50, 100, 200, 500]\n",
    "lrs = [0.1, 0.01, 0.001]\n",
    "batches = [100, 200, 500]\n",
    "dropouts = [0.1, 0.2, 0.5]\n",
    "#epochs = [5, 10, 15]\n",
    "repeats = 1\n",
    "model = [\"lstm\", \"vanilla\"]\n",
    "model = model[0]\n",
    "\n",
    "numComb = 1\n",
    "combs = []\n",
    "np.random.seed(42)\n",
    "for i in range(numComb):\n",
    "    combs.append([np.random.choice(lrs), np.random.choice(batches), np.random.choice(dropouts)])\n",
    "\n",
    "print(combs)\n",
    "\n",
    "tunehyps(states, combs, repeats, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
