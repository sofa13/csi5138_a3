{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CSI 5138 Homework 3**\n",
    "\n",
    "Use Vanilla RNN and LSTM to for text classification and sentiment analysis on a standard dataset of movie reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.models import Model\n",
    "#from keras import backend as K\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Movie Review Dataset, Test and Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processReviews(paths):\n",
    "    texts = []\n",
    "    ratings = []\n",
    "    \n",
    "    for path in paths:\n",
    "        for file in os.listdir(path):\n",
    "            # get review\n",
    "            rating = file.split('_')[1]\n",
    "            rating = rating.split('.')[0]\n",
    "            file = os.path.join(path, file)\n",
    "            with open(file, \"r\", encoding='utf-8') as f:\n",
    "                text = []\n",
    "                for line in f:\n",
    "                    # do some pre-processing and combine list of words for each review text             \n",
    "                    text += gensim.utils.simple_preprocess(line)\n",
    "                texts.append(text)\n",
    "                ratings.append(rating)\n",
    "        \n",
    "    return [texts, ratings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, ytrain = processReviews([\"./aclImdb/train/neg/\", \"./aclImdb/train/pos/\"])\n",
    "Xtest, ytest = processReviews([\"./aclImdb/test/neg/\", \"./aclImdb/test/pos/\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['story', 'of', 'man', 'who', 'has', 'unnatural', 'feelings', 'for', 'pig', 'starts', 'out', 'with', 'opening', 'scene', 'that', 'is', 'terrific', 'example', 'of', 'absurd', 'comedy', 'formal', 'orchestra', 'audience', 'is', 'turned', 'into', 'an', 'insane', 'violent', 'mob', 'by', 'the', 'crazy', 'chantings', 'of', 'it', 'singers', 'unfortunately', 'it', 'stays', 'absurd', 'the', 'whole', 'time', 'with', 'no', 'general', 'narrative', 'eventually', 'making', 'it', 'just', 'too', 'off', 'putting', 'even', 'those', 'from', 'the', 'era', 'should', 'be', 'turned', 'off', 'the', 'cryptic', 'dialogue', 'would', 'make', 'shakespeare', 'seem', 'easy', 'to', 'third', 'grader', 'on', 'technical', 'level', 'it', 'better', 'than', 'you', 'might', 'think', 'with', 'some', 'good', 'cinematography', 'by', 'future', 'great', 'vilmos', 'zsigmond', 'future', 'stars', 'sally', 'kirkland', 'and', 'frederic', 'forrest', 'can', 'be', 'seen', 'briefly']\n",
      "3\n",
      "['once', 'again', 'mr', 'costner', 'has', 'dragged', 'out', 'movie', 'for', 'far', 'longer', 'than', 'necessary', 'aside', 'from', 'the', 'terrific', 'sea', 'rescue', 'sequences', 'of', 'which', 'there', 'are', 'very', 'few', 'just', 'did', 'not', 'care', 'about', 'any', 'of', 'the', 'characters', 'most', 'of', 'us', 'have', 'ghosts', 'in', 'the', 'closet', 'and', 'costner', 'character', 'are', 'realized', 'early', 'on', 'and', 'then', 'forgotten', 'until', 'much', 'later', 'by', 'which', 'time', 'did', 'not', 'care', 'the', 'character', 'we', 'should', 'really', 'care', 'about', 'is', 'very', 'cocky', 'overconfident', 'ashton', 'kutcher', 'the', 'problem', 'is', 'he', 'comes', 'off', 'as', 'kid', 'who', 'thinks', 'he', 'better', 'than', 'anyone', 'else', 'around', 'him', 'and', 'shows', 'no', 'signs', 'of', 'cluttered', 'closet', 'his', 'only', 'obstacle', 'appears', 'to', 'be', 'winning', 'over', 'costner', 'finally', 'when', 'we', 'are', 'well', 'past', 'the', 'half', 'way', 'point', 'of', 'this', 'stinker', 'costner', 'tells', 'us', 'all', 'about', 'kutcher', 'ghosts', 'we', 'are', 'told', 'why', 'kutcher', 'is', 'driven', 'to', 'be', 'the', 'best', 'with', 'no', 'prior', 'inkling', 'or', 'foreshadowing', 'no', 'magic', 'here', 'it', 'was', 'all', 'could', 'do', 'to', 'keep', 'from', 'turning', 'it', 'off', 'an', 'hour', 'in']\n",
      "2\n",
      "['working', 'class', 'romantic', 'drama', 'from', 'director', 'martin', 'ritt', 'is', 'as', 'unbelievable', 'as', 'they', 'come', 'yet', 'there', 'are', 'moments', 'of', 'pleasure', 'due', 'mostly', 'to', 'the', 'charisma', 'of', 'stars', 'jane', 'fonda', 'and', 'robert', 'de', 'niro', 'both', 'terrific', 'she', 'widow', 'who', 'can', 'move', 'on', 'he', 'illiterate', 'and', 'closet', 'inventor', 'you', 'can', 'probably', 'guess', 'the', 'rest', 'adaptation', 'of', 'pat', 'barker', 'novel', 'union', 'street', 'better', 'title', 'is', 'so', 'laid', 'back', 'it', 'verges', 'on', 'bland', 'and', 'the', 'film', 'editing', 'is', 'mess', 'but', 'it', 'still', 'pleasant', 'rosy', 'hued', 'blue', 'collar', 'fantasy', 'there', 'are', 'no', 'overtures', 'to', 'serious', 'issues', 'even', 'the', 'illiteracy', 'angle', 'is', 'just', 'plot', 'tool', 'for', 'the', 'ensuing', 'love', 'story', 'and', 'no', 'real', 'fireworks', 'though', 'the', 'characters', 'are', 'intentionally', 'bit', 'colorless', 'and', 'the', 'leads', 'are', 'toned', 'down', 'to', 'an', 'interesting', 'degree', 'the', 'finale', 'is', 'pure', 'fluff', 'and', 'cynics', 'will', 'find', 'it', 'difficult', 'to', 'swallow', 'though', 'these', 'two', 'characters', 'deserve', 'happy', 'ending', 'and', 'the', 'picture', 'wouldn', 'really', 'be', 'satisfying', 'any', 'other', 'way', 'from']\n",
      "7\n",
      "['ve', 'seen', 'this', 'story', 'before', 'but', 'my', 'kids', 'haven', 'boy', 'with', 'troubled', 'past', 'joins', 'military', 'faces', 'his', 'past', 'falls', 'in', 'love', 'and', 'becomes', 'man', 'the', 'mentor', 'this', 'time', 'is', 'played', 'perfectly', 'by', 'kevin', 'costner', 'an', 'ordinary', 'man', 'with', 'common', 'everyday', 'problems', 'who', 'lives', 'an', 'extraordinary', 'conviction', 'to', 'save', 'lives', 'after', 'losing', 'his', 'team', 'he', 'takes', 'teaching', 'position', 'training', 'the', 'next', 'generation', 'of', 'heroes', 'the', 'young', 'troubled', 'recruit', 'is', 'played', 'by', 'kutcher', 'while', 'his', 'scenes', 'with', 'the', 'local', 'love', 'interest', 'are', 'tad', 'stiff', 'and', 'don', 'generate', 'enough', 'heat', 'to', 'melt', 'butter', 'he', 'compliments', 'costner', 'well', 'never', 'really', 'understood', 'sela', 'ward', 'as', 'the', 'neglected', 'wife', 'and', 'felt', 'she', 'should', 'of', 'wanted', 'costner', 'to', 'quit', 'out', 'of', 'concern', 'for', 'his', 'safety', 'as', 'opposed', 'to', 'her', 'selfish', 'needs', 'but', 'her', 'presence', 'on', 'screen', 'is', 'pleasure', 'the', 'two', 'unaccredited', 'stars', 'of', 'this', 'movie', 'are', 'the', 'coast', 'guard', 'and', 'the', 'sea', 'both', 'powerful', 'forces', 'which', 'should', 'not', 'be', 'taken', 'for', 'granted', 'in', 'real', 'life', 'or', 'this', 'movie', 'the', 'movie', 'has', 'some', 'slow', 'spots', 'and', 'could', 'have', 'used', 'the', 'wasted', 'minutes', 'to', 'strengthen', 'the', 'character', 'relationships', 'but', 'it', 'still', 'works', 'the', 'rescue', 'scenes', 'are', 'intense', 'and', 'well', 'filmed', 'and', 'edited', 'to', 'provide', 'maximum', 'impact', 'this', 'movie', 'earns', 'the', 'audience', 'applause', 'and', 'the', 'applause', 'of', 'my', 'two', 'sons']\n",
      "7\n",
      "# Xtrain:  25000\n",
      "# ytrain:  25000\n",
      "# Xtest:  25000\n",
      "# ytest:  25000\n"
     ]
    }
   ],
   "source": [
    "print(Xtrain[0])\n",
    "print(ytrain[0])\n",
    "print(Xtest[0])\n",
    "print(ytest[0])\n",
    "print(Xtrain[-1])\n",
    "print(ytrain[-1])\n",
    "print(Xtest[-1])\n",
    "print(ytest[-1])\n",
    "print(\"# Xtrain: \", len(Xtrain))\n",
    "print(\"# ytrain: \", len(ytrain))\n",
    "print(\"# Xtest: \", len(Xtest))\n",
    "print(\"# ytest: \", len(ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# X:  50000\n",
      "# y:  50000\n"
     ]
    }
   ],
   "source": [
    "X = list(Xtrain + Xtest)\n",
    "y = list(ytrain + ytest)\n",
    "print(\"# X: \", len(X))\n",
    "print(\"# y: \", len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Keras with Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 99476 unique tokens.\n",
      "Shape of data tensor: (50000, 2380)\n",
      "Shape of label tensor: (50000, 11)\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences)\n",
    "\n",
    "labels = to_categorical(np.asarray(y))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "print(labels[0])\n",
    "# review on a 1-10 scale\n",
    "# indices 0-3 are neg reviews <= 4, indices 6-9 are pos reviews >=7\n",
    "labels = labels[:,1:]\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = data[:25000], labels[:25000]\n",
    "x_val, y_val = data[25000:], labels[25000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# x_train:  25000\n",
      "# x_test:  25000\n"
     ]
    }
   ],
   "source": [
    "print(\"# x_train: \", len(x_train))\n",
    "print(\"# x_test: \", len(x_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing train and test separetly\n",
    "# train\n",
    "# Found 73293 unique tokens.\n",
    "# Shape of data tensor: (25000, 2380)\n",
    "# Shape of label tensor: (25000, 11)\n",
    "# [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
    "\n",
    "# test\n",
    "# Found 72280 unique tokens.\n",
    "# Shape of data tensor: (25000, 2265)\n",
    "# Shape of label tensor: (25000, 11)\n",
    "# [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "glove_file = './glove.6B/glove.6B.100d.txt'\n",
    "\n",
    "with open(glove_file, \"r\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM=100\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99476\n"
     ]
    }
   ],
   "source": [
    "print(len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "682\n",
      "1577\n",
      "[-0.32306999 -0.87616003  0.21977     0.25268     0.22976001  0.73879999\n",
      " -0.37954    -0.35306999 -0.84368998 -1.11129999 -0.30265999  0.33177999\n",
      " -0.25113001  0.30447999 -0.077491   -0.89815003  0.092496   -1.14069998\n",
      " -0.58323997  0.66869003 -0.23122001 -0.95854998  0.28262001 -0.078848\n",
      "  0.75314999  0.26583999  0.34220001 -0.33949     0.95608002  0.065641\n",
      "  0.45747     0.39835     0.57964998  0.39267001 -0.21851     0.58794999\n",
      " -0.55998999  0.63367999 -0.043983   -0.68730998 -0.37841001  0.38025999\n",
      "  0.61641002 -0.88269001 -0.12346    -0.37928    -0.38317999  0.23868001\n",
      "  0.66850001 -0.43320999 -0.11065     0.081723    1.15690005  0.78957999\n",
      " -0.21223    -2.3211     -0.67806     0.44560999  0.65706998  0.1045\n",
      "  0.46217     0.19912     0.25802001  0.057194    0.53443003 -0.43133\n",
      " -0.34311     0.59789002 -0.58416998  0.068995    0.23943999 -0.85180998\n",
      "  0.30379    -0.34176999 -0.25746    -0.031101   -0.16285001  0.45168999\n",
      " -0.91627002  0.64521003  0.73281002 -0.22752     0.30226001  0.044801\n",
      " -0.83740997  0.55005997 -0.52506    -1.73570001  0.47510001 -0.70486999\n",
      "  0.056939   -0.71319997  0.089623    0.41394001 -1.33630002 -0.61914998\n",
      " -0.33089    -0.52881002  0.16483    -0.98878002]\n",
      "[-0.50045002 -0.70826     0.55387998  0.67299998  0.22486     0.60281003\n",
      " -0.26194     0.73872    -0.65382999 -0.21606    -0.33805999  0.24498001\n",
      " -0.51497     0.85680002 -0.37199    -0.58824003  0.30636999 -0.30667999\n",
      " -0.21870001  0.78368998 -0.61944002 -0.54925001  0.43066999 -0.027348\n",
      "  0.97574002  0.46169001  0.11486    -0.99842     1.0661     -0.20818999\n",
      "  0.53157997  0.40922001  1.04059994  0.24943     0.18708999  0.41528001\n",
      " -0.95407999  0.36822    -0.37948    -0.68019998 -0.14578    -0.20113\n",
      "  0.17113    -0.55704999  0.7191      0.070014   -0.23637     0.49533999\n",
      "  1.15760005 -0.05078     0.25731    -0.091052    1.26629996  1.10469997\n",
      " -0.51583999 -2.00329995 -0.64820999  0.16417     0.32934999  0.048484\n",
      "  0.18997     0.66115999  0.080882    0.3364      0.22758     0.1462\n",
      " -0.51005     0.63777     0.47299001 -0.32820001  0.083899   -0.78547001\n",
      "  0.099148    0.039176    0.27893001  0.11747     0.57862002  0.043639\n",
      " -0.15965    -0.35304001 -0.048965   -0.32460999  1.49810004  0.58138001\n",
      " -1.13199997 -0.60672998 -0.37505001 -1.18130004  0.80116999 -0.50014001\n",
      " -0.16574    -0.70583999  0.43011999  0.51051003 -0.80330002 -0.66571999\n",
      " -0.63717002 -0.36032     0.13347    -0.56075001]\n",
      "[ 5.69510013e-02 -1.19580003e-02  4.59490001e-01 -4.02049989e-01\n",
      "  1.14320002e-01  6.42979980e-01 -1.81079999e-01 -5.95189989e-01\n",
      "  1.64230004e-01  2.58290004e-02 -8.48150015e-01 -5.19829988e-01\n",
      "  3.03310007e-01 -2.15579998e-02  2.05369994e-01  6.19440019e-01\n",
      "  7.26710021e-01  6.90999985e-01  3.04930001e-01  1.23510003e+00\n",
      "  4.25269991e-01 -7.85809994e-01 -1.81040004e-01 -3.32910001e-01\n",
      "  1.01040006e+00 -4.74339984e-02  1.31789995e-02 -4.68309999e-01\n",
      " -2.72879988e-01  5.32689989e-01  1.02480002e-01  8.70010018e-01\n",
      " -9.47140008e-02 -3.77970010e-01  3.40090007e-01  2.43460000e-01\n",
      "  1.53850004e-01  1.28260002e-01  7.48390034e-02  7.15419999e-04\n",
      " -1.05999999e-01  1.46779999e-01  2.84310013e-01  6.17770016e-01\n",
      " -1.52630001e-01  3.60780001e-01 -6.44620001e-01 -1.75239995e-01\n",
      "  2.52689987e-01 -3.07559997e-01 -7.16890022e-02  8.78169984e-02\n",
      "  1.01289999e+00  1.64030004e+00  7.21080005e-01 -2.33640003e+00\n",
      "  1.92699991e-02 -1.76630005e-01 -5.50439991e-02  3.03030014e-01\n",
      "  4.25139993e-01  1.40980005e+00 -3.57030004e-01  2.55650014e-01\n",
      "  1.58070004e+00 -3.67940009e-01  6.44959986e-01 -8.43930021e-02\n",
      "  3.04540005e-02  6.58599973e-01 -9.90699977e-02 -6.64670020e-02\n",
      " -6.36359975e-02 -6.07060008e-02  5.33330023e-01  9.22290012e-02\n",
      " -1.42480001e-01  1.58960000e-01 -1.28849998e-01 -2.73330003e-01\n",
      " -7.67379999e-01  1.32560000e-01 -2.04999998e-01 -1.61489993e-01\n",
      " -1.57690001e+00 -3.16010006e-02 -7.47950017e-01 -2.22340003e-01\n",
      "  3.10409993e-01 -7.10749984e-01 -1.94670007e-01  2.97850013e-01\n",
      "  2.10590005e-01  3.16949993e-01  5.45869991e-02 -7.65120015e-02\n",
      " -4.41740006e-01 -1.10539997e+00 -1.48029998e-01  3.02749991e-01]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#print(list(word_index.items())[100])\n",
    "print(word_index['king'])\n",
    "print(word_index['queen'])\n",
    "print(embedding_matrix[682])\n",
    "print(embedding_matrix[1577])\n",
    "print(embedding_matrix[101])\n",
    "print(embedding_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 100\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(35)(x)  # global max pooling\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "# happy learning!\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          epochs=2, batch_size=128)\n",
    "score = model.evaluate(x_val, y_val, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Word2Vec with Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 100)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_input_file = './glove.6B/glove.6B.100d.txt'\n",
    "word2vec_output_file = './glove.6B/glove.6B.100d.txt.word2vec'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.7698541283607483)]\n"
     ]
    }
   ],
   "source": [
    "# load the Stanford GloVe model\n",
    "filename = './glove.6B/glove.6B.100d.txt.word2vec'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
    "# calculate: (king - man) + woman = ?\n",
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x000001AF85C80588>\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "#print(list(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "             ['this', 'is', 'the', 'second', 'sentence'],\n",
    "             ['yet', 'another', 'sentence'],\n",
    "             ['one', 'more', 'sentence'],\n",
    "             ['and', 'the', 'final', 'sentence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(model['samsung']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
